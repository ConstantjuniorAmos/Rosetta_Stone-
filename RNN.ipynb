{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import load_data\n",
    "import numpy as np\n",
    "import functions as tests\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Dropout, LSTM, Embedding, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English data\n",
    "english_sentences = load_data.load_data('src/small_vocab_en.txt')\n",
    "# Load French data\n",
    "french_sentences = load_data.load_data('src/small_vocab_fr.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sample 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "French sample 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "English sample 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "French sample 2:  les Ã©tats-unis est gÃ©nÃ©ralement froid en juillet , et il gÃ¨le habituellement en novembre .\n",
      "\n",
      "English sample 3:  california is usually quiet during march , and it is usually hot in june .\n",
      "French sample 3:  california est gÃ©nÃ©ralement calme en mars , et il est gÃ©nÃ©ralement chaud en juin .\n",
      "\n",
      "English sample 4:  the united states is sometimes mild during june , and it is cold in september .\n",
      "French sample 4:  les Ã©tats-unis est parfois lÃ©gÃ¨re en juin , et il fait froid en septembre .\n",
      "\n",
      "English sample 5:  your least liked fruit is the grape , but my least liked is the apple .\n",
      "French sample 5:  votre moins aimÃ© fruit est le raisin , mais mon moins aimÃ© est la pomme .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(5):\n",
    "    print('English sample {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('French sample {}:  {}\\n'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    return pad_sequences(x, maxlen=length, padding='post')\n",
    "\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 345\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = ''\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_3 (GRU)                 (None, 21, 256)           198912    \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 21, 1024)         263168    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 21, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 21, 346)          354650    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 816,730\n",
      "Trainable params: 816,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - 90s 823ms/step - loss: 1.9377 - accuracy: 0.5434 - val_loss: 1.2842 - val_accuracy: 0.6377\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 89s 823ms/step - loss: 1.2324 - accuracy: 0.6437 - val_loss: 1.0812 - val_accuracy: 0.6781\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 89s 827ms/step - loss: 1.0856 - accuracy: 0.6703 - val_loss: 0.9827 - val_accuracy: 0.6846\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 89s 826ms/step - loss: 0.9992 - accuracy: 0.6850 - val_loss: 0.9094 - val_accuracy: 0.7057\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 96s 888ms/step - loss: 0.9409 - accuracy: 0.6963 - val_loss: 0.8605 - val_accuracy: 0.7169\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 95s 884ms/step - loss: 0.8892 - accuracy: 0.7076 - val_loss: 0.8101 - val_accuracy: 0.7299\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 95s 884ms/step - loss: 0.8471 - accuracy: 0.7190 - val_loss: 0.7900 - val_accuracy: 0.7335\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 103s 958ms/step - loss: 0.8244 - accuracy: 0.7243 - val_loss: 0.8166 - val_accuracy: 0.7124\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 115s 1s/step - loss: 0.8336 - accuracy: 0.7129 - val_loss: 0.7446 - val_accuracy: 0.7373\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 120s 1s/step - loss: 0.7878 - accuracy: 0.7289 - val_loss: 0.6998 - val_accuracy: 0.7602\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "new jersey est parfois chaud en mois de il et il est en en       \n"
     ]
    }
   ],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # TODO: Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size+1, activation='softmax'))) \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#tests.test_simple_model(simple_model)\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "print(simple_rnn_model.summary())\n",
    "\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "new jersey est parfois chaud en mois de il et il est en en       \n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 21, 256)           51200     \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 21, 256)           394752    \n",
      "                                                                 \n",
      " time_distributed_10 (TimeDi  (None, 21, 1024)         263168    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 21, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_11 (TimeDi  (None, 21, 346)          354650    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,063,770\n",
      "Trainable params: 1,063,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - 119s 1s/step - loss: 1.4023 - accuracy: 0.6738 - val_loss: 0.5201 - val_accuracy: 0.8341\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 120s 1s/step - loss: 0.4245 - accuracy: 0.8614 - val_loss: 0.3162 - val_accuracy: 0.8933\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 119s 1s/step - loss: 0.3043 - accuracy: 0.8979 - val_loss: 0.2630 - val_accuracy: 0.9112\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 119s 1s/step - loss: 0.2524 - accuracy: 0.9143 - val_loss: 0.2252 - val_accuracy: 0.9228\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 119s 1s/step - loss: 0.2228 - accuracy: 0.9234 - val_loss: 0.2030 - val_accuracy: 0.9298\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 118s 1s/step - loss: 0.2079 - accuracy: 0.9279 - val_loss: 0.1963 - val_accuracy: 0.9312\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 119s 1s/step - loss: 0.1982 - accuracy: 0.9309 - val_loss: 0.1893 - val_accuracy: 0.9340\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 117s 1s/step - loss: 0.1909 - accuracy: 0.9328 - val_loss: 0.1862 - val_accuracy: 0.9346\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 109s 1s/step - loss: 0.1842 - accuracy: 0.9346 - val_loss: 0.1833 - val_accuracy: 0.9354\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 113s 1s/step - loss: 0.1821 - accuracy: 0.9351 - val_loss: 0.1843 - val_accuracy: 0.9356\n",
      "1/1 [==============================] - 0s 317ms/step\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril       \n"
     ]
    }
   ],
   "source": [
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # TODO: Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
    "    model.add(GRU(256, return_sequences=True))    \n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# tests.test_embed_model(embed_model)\n",
    "\n",
    "# TODO: Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "# TODO: Train the neural network\n",
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "embed_rnn_model.summary()\n",
    "\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# TODO: Print prediction(s)\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril       \n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 21, 256)           51200     \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 21, 256)           394752    \n",
      "                                                                 \n",
      " time_distributed_12 (TimeDi  (None, 21, 1024)         263168    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 21, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_13 (TimeDi  (None, 21, 346)          354650    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,063,770\n",
      "Trainable params: 1,063,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - 123s 1s/step - loss: 1.3422 - accuracy: 0.6874 - val_loss: 0.4787 - val_accuracy: 0.8422\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 120s 1s/step - loss: 0.4043 - accuracy: 0.8671 - val_loss: 0.3003 - val_accuracy: 0.8987\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 120s 1s/step - loss: 0.2920 - accuracy: 0.9019 - val_loss: 0.2441 - val_accuracy: 0.9174\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 117s 1s/step - loss: 0.2439 - accuracy: 0.9171 - val_loss: 0.2193 - val_accuracy: 0.9253\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 118s 1s/step - loss: 0.2194 - accuracy: 0.9246 - val_loss: 0.2057 - val_accuracy: 0.9286\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 122s 1s/step - loss: 0.2054 - accuracy: 0.9288 - val_loss: 0.1919 - val_accuracy: 0.9331\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 122s 1s/step - loss: 0.1942 - accuracy: 0.9320 - val_loss: 0.1882 - val_accuracy: 0.9347\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 118s 1s/step - loss: 0.1887 - accuracy: 0.9336 - val_loss: 0.1867 - val_accuracy: 0.9349\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 125s 1s/step - loss: 0.1831 - accuracy: 0.9350 - val_loss: 0.1827 - val_accuracy: 0.9356\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 116s 1s/step - loss: 0.1812 - accuracy: 0.9355 - val_loss: 0.1809 - val_accuracy: 0.9363\n",
      "1/1 [==============================] - 0s 295ms/step\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril       \n"
     ]
    }
   ],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a bidirectional RNN model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.003\n",
    "    \n",
    "    # TODO: Build the layers\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True), input_shape=input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# tests.test_bd_model(bd_model)\n",
    "\n",
    "# TODO: Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "# TODO: Train and Print prediction(s)\n",
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "embed_rnn_model.summary()\n",
    "\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril       \n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_7 (GRU)                 (None, 256)               198912    \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 21, 256)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " gru_8 (GRU)                 (None, 21, 256)           394752    \n",
      "                                                                 \n",
      " time_distributed_14 (TimeDi  (None, 21, 1024)         263168    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 21, 1024)          0         \n",
      "                                                                 \n",
      " time_distributed_15 (TimeDi  (None, 21, 346)          354650    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,211,482\n",
      "Trainable params: 1,211,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "108/108 [==============================] - 165s 1s/step - loss: 2.4742 - accuracy: 0.4722 - val_loss: 1.7977 - val_accuracy: 0.5681\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 174s 2s/step - loss: 1.6584 - accuracy: 0.5743 - val_loss: 1.4642 - val_accuracy: 0.6103\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 174s 2s/step - loss: 1.4361 - accuracy: 0.6089 - val_loss: 1.3425 - val_accuracy: 0.6336\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 154s 1s/step - loss: 1.3334 - accuracy: 0.6314 - val_loss: 1.2642 - val_accuracy: 0.6498\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 143s 1s/step - loss: 1.2697 - accuracy: 0.6452 - val_loss: 1.2082 - val_accuracy: 0.6559\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 143s 1s/step - loss: 1.2138 - accuracy: 0.6561 - val_loss: 1.1477 - val_accuracy: 0.6703\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 142s 1s/step - loss: 1.1765 - accuracy: 0.6624 - val_loss: 1.1096 - val_accuracy: 0.6754\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 142s 1s/step - loss: 1.1201 - accuracy: 0.6742 - val_loss: 1.0724 - val_accuracy: 0.6843\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 142s 1s/step - loss: 1.0942 - accuracy: 0.6797 - val_loss: 1.0362 - val_accuracy: 0.6954\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 143s 1s/step - loss: 1.0823 - accuracy: 0.6830 - val_loss: 1.0151 - val_accuracy: 0.7015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2054fee9280>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # OPTIONAL: Implement\n",
    "    \n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Build the layers    \n",
    "    model = Sequential()\n",
    "    # Encoder\n",
    "    model.add(GRU(256, input_shape=input_shape[1:], go_backwards=True))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    # Decoder\n",
    "    model.add(GRU(256, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# tests.test_encdec_model(encdec_model)\n",
    "\n",
    "# Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train and Print prediction(s)\n",
    "encdec_rnn_model = encdec_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "encdec_rnn_model.summary()\n",
    "\n",
    "encdec_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\n",
      "1/1 [==============================] - 1s 538ms/step\n",
      "new jersey est parfois chaud en l' et il est est en en        \n",
      "\n",
      "Correct Translation:\n",
      "[\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\"]\n",
      "\n",
      "Original text:\n",
      "['new jersey is sometimes quiet during autumn , and it is snowy in april .']\n"
     ]
    }
   ],
   "source": [
    "# Print prediction(s)\n",
    "print(\"Prediction:\")\n",
    "print(logits_to_text(encdec_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "\n",
    "print(\"\\nCorrect Translation:\")\n",
    "print(french_sentences[:1])\n",
    "\n",
    "print(\"\\nOriginal text:\")\n",
    "print(english_sentences[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Loaded\n"
     ]
    }
   ],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.003\n",
    "    \n",
    "    # Build the layers    \n",
    "    model = Sequential()\n",
    "    # Embedding\n",
    "    model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1],\n",
    "                         input_shape=input_shape[1:]))\n",
    "    # Encoder\n",
    "    model.add(Bidirectional(GRU(128)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    # Decoder\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "    model.add(TimeDistributed(Dense(512, activation='relu')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# tests.test_model_final(model_final)\n",
    "\n",
    "print('Final Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 15, 128)           25600     \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 256)              198144    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " repeat_vector_2 (RepeatVect  (None, 21, 256)          0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 21, 256)          296448    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_18 (TimeDi  (None, 21, 512)          131584    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 21, 512)           0         \n",
      "                                                                 \n",
      " time_distributed_19 (TimeDi  (None, 21, 346)          177498    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 829,274\n",
      "Trainable params: 829,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "108/108 [==============================] - 149s 1s/step - loss: 2.5111 - accuracy: 0.4753 - val_loss: 1.6039 - val_accuracy: 0.5878\n",
      "Epoch 2/25\n",
      "108/108 [==============================] - 142s 1s/step - loss: 1.4498 - accuracy: 0.6155 - val_loss: 1.1838 - val_accuracy: 0.6733\n",
      "Epoch 3/25\n",
      "108/108 [==============================] - 145s 1s/step - loss: 1.1262 - accuracy: 0.6837 - val_loss: 0.9871 - val_accuracy: 0.7198\n",
      "Epoch 4/25\n",
      "108/108 [==============================] - 162s 1s/step - loss: 0.9544 - accuracy: 0.7214 - val_loss: 0.7870 - val_accuracy: 0.7638\n",
      "Epoch 5/25\n",
      "108/108 [==============================] - 152s 1s/step - loss: 0.8970 - accuracy: 0.7322 - val_loss: 0.7202 - val_accuracy: 0.7798\n",
      "Epoch 6/25\n",
      "108/108 [==============================] - 147s 1s/step - loss: 0.7047 - accuracy: 0.7796 - val_loss: 0.5564 - val_accuracy: 0.8253\n",
      "Epoch 7/25\n",
      "108/108 [==============================] - 143s 1s/step - loss: 0.5763 - accuracy: 0.8170 - val_loss: 0.4453 - val_accuracy: 0.8608\n",
      "Epoch 8/25\n",
      "108/108 [==============================] - 156s 1s/step - loss: 0.4855 - accuracy: 0.8452 - val_loss: 0.3718 - val_accuracy: 0.8854\n",
      "Epoch 9/25\n",
      "108/108 [==============================] - 154s 1s/step - loss: 0.4280 - accuracy: 0.8642 - val_loss: 0.3043 - val_accuracy: 0.9074\n",
      "Epoch 10/25\n",
      "108/108 [==============================] - 158s 1s/step - loss: 0.3534 - accuracy: 0.8918 - val_loss: 0.3452 - val_accuracy: 0.8950\n",
      "Epoch 11/25\n",
      "108/108 [==============================] - 150s 1s/step - loss: 0.2882 - accuracy: 0.9132 - val_loss: 0.1972 - val_accuracy: 0.9443\n",
      "Epoch 12/25\n",
      "108/108 [==============================] - 141s 1s/step - loss: 0.2378 - accuracy: 0.9295 - val_loss: 0.1722 - val_accuracy: 0.9510\n",
      "Epoch 13/25\n",
      "108/108 [==============================] - 141s 1s/step - loss: 0.2303 - accuracy: 0.9317 - val_loss: 0.2183 - val_accuracy: 0.9324\n",
      "Epoch 14/25\n",
      "108/108 [==============================] - 138s 1s/step - loss: 0.1946 - accuracy: 0.9422 - val_loss: 0.1404 - val_accuracy: 0.9595\n",
      "Epoch 15/25\n",
      "108/108 [==============================] - 136s 1s/step - loss: 0.1659 - accuracy: 0.9508 - val_loss: 0.1224 - val_accuracy: 0.9641\n",
      "Epoch 16/25\n",
      "108/108 [==============================] - 151s 1s/step - loss: 0.1506 - accuracy: 0.9553 - val_loss: 0.1190 - val_accuracy: 0.9648\n",
      "Epoch 17/25\n",
      "108/108 [==============================] - 156s 1s/step - loss: 0.1391 - accuracy: 0.9586 - val_loss: 0.1101 - val_accuracy: 0.9669\n",
      "Epoch 18/25\n",
      "108/108 [==============================] - 155s 1s/step - loss: 0.1260 - accuracy: 0.9622 - val_loss: 0.1024 - val_accuracy: 0.9696\n",
      "Epoch 19/25\n",
      "108/108 [==============================] - 161s 1s/step - loss: 0.1184 - accuracy: 0.9645 - val_loss: 0.0937 - val_accuracy: 0.9726\n",
      "Epoch 20/25\n",
      "108/108 [==============================] - 161s 1s/step - loss: 0.1201 - accuracy: 0.9643 - val_loss: 0.0944 - val_accuracy: 0.9721\n",
      "Epoch 21/25\n",
      "108/108 [==============================] - 160s 1s/step - loss: 0.1070 - accuracy: 0.9678 - val_loss: 0.0900 - val_accuracy: 0.9735\n",
      "Epoch 22/25\n",
      "108/108 [==============================] - 158s 1s/step - loss: 0.1063 - accuracy: 0.9680 - val_loss: 0.0868 - val_accuracy: 0.9747\n",
      "Epoch 23/25\n",
      "108/108 [==============================] - 142s 1s/step - loss: 0.0932 - accuracy: 0.9718 - val_loss: 0.0896 - val_accuracy: 0.9752\n",
      "Epoch 24/25\n",
      "108/108 [==============================] - 151s 1s/step - loss: 0.1008 - accuracy: 0.9698 - val_loss: 0.1284 - val_accuracy: 0.9651\n",
      "Epoch 25/25\n",
      "108/108 [==============================] - 149s 1s/step - loss: 0.1168 - accuracy: 0.9656 - val_loss: 0.0998 - val_accuracy: 0.9711\n",
      "WARNING:tensorflow:6 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020508885160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Sample 1:\n",
      "il a vu un vieux camion jaune              \n",
      "Il a vu un vieux camion jaune\n",
      "Sample 2:\n",
      "new jersey est parfois calme ã  l'automne et il est neigeux en avril        \n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril       \n"
     ]
    }
   ],
   "source": [
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    # TODO: Train neural network using model_final\n",
    "    model = model_final(x.shape,y.shape[1],\n",
    "                        len(x_tk.word_index)+1,\n",
    "                        len(y_tk.word_index)+1)\n",
    "    model.summary()\n",
    "    model.fit(x, y, batch_size=1024, epochs=25, validation_split=0.2)\n",
    "\n",
    "    model.save('model.h5')\n",
    "    \n",
    "    ## DON'T EDIT ANYTHING BELOW THIS LINE\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = ''\n",
    "\n",
    "    sentence = 'he saw a old yellow truck'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    print('Il a vu un vieux camion jaune')\n",
    "    print('Sample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
    "\n",
    "\n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
